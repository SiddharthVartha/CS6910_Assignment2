{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "vCFDgD1ASH4n",
        "outputId": "ca9b8f55-729a-41b4-a717-a9c214c6f6ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.44.0-py2.py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "wandb.login()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_data_loaders(train_data_dir, test_data_dir, data_augmentation):\n",
        "    # Define default transform\n",
        "    default_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Define transform for data augmentation\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomResizedCrop(256),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load the dataset without augmentation\n",
        "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=default_transform)\n",
        "\n",
        "    # Separate classes\n",
        "    class_to_idx = train_dataset.class_to_idx\n",
        "    idx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\n",
        "\n",
        "    # Create a dictionary to hold data indices for each class\n",
        "    class_indices = defaultdict(list)\n",
        "    for idx, (_, label) in enumerate(train_dataset.samples):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    # Set aside 20% of data from each class for validation\n",
        "    validation_indices = []\n",
        "    train_indices = []\n",
        "    for class_idx, indices in class_indices.items():\n",
        "        num_validation = int(0.2 * len(indices))\n",
        "        random.shuffle(indices)  # Shuffle indices to ensure randomness\n",
        "        validation_indices.extend(indices[:num_validation])\n",
        "        train_indices.extend(indices[num_validation:])\n",
        "\n",
        "    # Create PyTorch data loaders for the initial dataset\n",
        "    train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=32, shuffle=True)\n",
        "    validation_loader = DataLoader(Subset(train_dataset, validation_indices), batch_size=32, shuffle=True)\n",
        "\n",
        "    # Load test data\n",
        "    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=default_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # If data augmentation is enabled, create augmented data loader\n",
        "    if data_augmentation:\n",
        "        # Create DataLoader for augmented training data using train_indices\n",
        "        augmented_dataset = datasets.ImageFolder(root=train_data_dir, transform=augment_transform)\n",
        "        augmented_loader = DataLoader(Subset(augmented_dataset, train_indices), batch_size=32, shuffle=True)\n",
        "        # Combine original training data and augmented data\n",
        "        combined_dataset = ConcatDataset([train_loader.dataset, augmented_loader.dataset])\n",
        "        train_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    return train_loader, validation_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWXkigEsS7J7"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs, device):\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        epoch_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "        for images, labels in epoch_progress:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_predictions += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            epoch_progress.set_postfix({'Loss': running_loss / len(train_loader), 'Accuracy': 100 * correct_predictions / total_predictions})\n",
        "        epoch_progress.close()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validation_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        val_loss /= len(validation_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        accuracy = 100 * correct_predictions / total_predictions\n",
        "        loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], accuracy: {accuracy:.2f}, loss: {loss:.2f}, val_accuracy: {val_accuracy:.2f}, val_loss: {val_loss:.2f}\")\n",
        "\n",
        "        # Log to Weights & Biases\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Accuracy\": round(accuracy, 2),\n",
        "            \"Loss\": round(loss, 2),\n",
        "            \"Validation Accuracy\": round(val_accuracy, 2),\n",
        "            \"Validation Loss\": round(val_loss, 2)\n",
        "        })\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vseE85qTLIX"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hXq1GCPTM85"
      },
      "outputs": [],
      "source": [
        "def CNN(input_channels, num_classes, num_filters, filter_size, activation, batch_norm, dropout, filter_multiplier, dense_neurons):\n",
        "    # Define activation function\n",
        "    if activation == 'ReLU':\n",
        "        act = nn.ReLU()\n",
        "    elif activation == 'GELU':\n",
        "        act = nn.GELU()\n",
        "    elif activation == 'SiLU':\n",
        "        act = nn.SiLU()\n",
        "    elif activation == 'Mish':\n",
        "        act = nn.Mish()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid activation function. Choose from 'relu', 'gelu', 'silu', or 'mish'.\")\n",
        "\n",
        "    layers = []\n",
        "    in_channels = input_channels\n",
        "    for _ in range(5):\n",
        "        layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=filter_size, stride=1, padding=1))\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm2d(num_filters))\n",
        "        layers.append(act)\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        in_channels = num_filters\n",
        "        num_filters =int(num_filters * filter_multiplier)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        *layers,\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_channels * 8 * 8, dense_neurons),\n",
        "        act,\n",
        "        nn.Linear(dense_neurons, num_classes)\n",
        "    )\n",
        "\n",
        "    return model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbnKlkt7TOmi"
      },
      "outputs": [],
      "source": [
        "# Define the wandb configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_accuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"num_filters\": {\"values\": [32, 64, 128]},\n",
        "        \"activation\": {\"values\": ['ReLU', 'GELU', 'SiLU', 'Mish']},\n",
        "        \"filter_size\": {\"values\": [2, 3, 5]},\n",
        "        \"filter_multiplier\": {\"values\": [1, 2, 0.5]},\n",
        "        \"data_augmentation\": {\"values\": ['No']},\n",
        "        \"batch_normalization\": {\"values\": ['Yes', 'No']},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"dense_neurons\": {\"values\": [128, 256, 512, 1024]},\n",
        "        \"epoch\":{\"values\": [5]},\n",
        "        \"learning_rate\":{\"values\":[0.001,0.0001]}\n",
        "    }\n",
        "}\n",
        "\n",
        "def train():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    # Set your hyperparameters from wandb config\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = f'num_filters_{config.num_filters}_activation_{config.activation}_filter_size_{config.filter_size}_filter_multiplier_{config.filter_multiplier}_data_augmentation_{config.data_augmentation}_batch_normalization_{config.batch_normalization}_dropout_{config.dropout}_dense_neurons_{config.dense_neurons}_learning_rate_{config.learning_rate}_epoch_{config.epoch}'\n",
        "\n",
        "    data_augmentation=False\n",
        "    if(config.data_augmentation=='Yes'):\n",
        "        data_augmentation=True\n",
        "\n",
        "    batch_normalization=False\n",
        "    if(config.batch_normalization=='Yes'):\n",
        "        batch_normalization=True\n",
        "\n",
        "    # Example usage:\n",
        "    train_data_dir = \"/content/drive/MyDrive/Deep Learning/A2/inaturalist_12K/train\"\n",
        "    test_data_dir = \"/content/drive/MyDrive/Deep Learning/A2/inaturalist_12K/val\"\n",
        "    train_loader, validation_loader, test_loader = get_data_loaders(train_data_dir, test_data_dir, data_augmentation)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation set size: {len(validation_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "    # Example usage:\n",
        "    model = CNN(\n",
        "        input_channels=3,\n",
        "        num_classes=10,\n",
        "        num_filters=config.num_filters,\n",
        "        filter_size=config.filter_size,\n",
        "        activation=config.activation,\n",
        "        batch_norm=batch_normalization,\n",
        "        dropout=config.dropout,\n",
        "        filter_multiplier=config.filter_multiplier,\n",
        "        dense_neurons=config.dense_neurons\n",
        "    )\n",
        "\n",
        "    #model = CNN(input_channels=3, num_classes=10, num_filters=32, filter_size=3, activation='mish', batch_norm=True, dropout=0.2, filter_multiplier=1, dense_neurons=256)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    model=train_model(model, train_loader, validation_loader, criterion, optimizer, config.epoch, device)\n",
        "\n",
        "    #Evaluate on Test data\n",
        "    evaluate_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "2a3f694dcd954eb48a2ff1f0589b507e"
          ]
        },
        "id": "r07plzi7TRjv",
        "outputId": "365537ec-fad9-43b3-f94d-249718063c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ohjskzur\n",
            "Sweep URL: https://wandb.ai/cs23m063/deep_learn_assignment_2/sweeps/ohjskzur\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eweshy2m with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: SiLU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: No\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_multiplier: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m063\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240402_102813-eweshy2m</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m063/deep_learn_assignment_2/runs/eweshy2m/workspace' target=\"_blank\">summer-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2/sweeps/ohjskzur' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_2/sweeps/ohjskzur</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2/sweeps/ohjskzur' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_2/sweeps/ohjskzur</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2/runs/eweshy2m/workspace' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_2/runs/eweshy2m/workspace</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 8000\n",
            "Validation set size: 1999\n",
            "Test set size: 2008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], accuracy: 19.95, loss: 2.19, val_accuracy: 20.96, val_loss: 2.17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], accuracy: 28.74, loss: 2.02, val_accuracy: 26.36, val_loss: 2.13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], accuracy: 31.23, loss: 1.93, val_accuracy: 30.62, val_loss: 1.99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5], accuracy: 34.55, loss: 1.84, val_accuracy: 31.42, val_loss: 1.98\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5], accuracy: 38.36, loss: 1.74, val_accuracy: 31.82, val_loss: 2.01\n",
            "Accuracy on test set: 32.87%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a3f694dcd954eb48a2ff1f0589b507e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▄▅▇█</td></tr><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Loss</td><td>█▅▄▃▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▇██</td></tr><tr><td>Validation Loss</td><td>█▇▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>38.36</td></tr><tr><td>Epoch</td><td>5</td></tr><tr><td>Loss</td><td>1.74</td></tr><tr><td>Validation Accuracy</td><td>31.82</td></tr><tr><td>Validation Loss</td><td>2.01</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">summer-sweep-1</strong> at: <a href='https://wandb.ai/cs23m063/deep_learn_assignment_2/runs/eweshy2m/workspace' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_2/runs/eweshy2m/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240402_102813-eweshy2m/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize wandb sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learn_assignment_2\" ,entity=\"cs23m063\")\n",
        "\n",
        "# Run wandb agent to execute the sweep\n",
        "wandb.agent(sweep_id, function=train, count =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ojvu4ZYTTO0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pCV8Oe3V4fSbpnpp_4dqpvuBQOsNpF4i",
      "authorship_tag": "ABX9TyOEmvENoWz62qnJP920UVyH"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}