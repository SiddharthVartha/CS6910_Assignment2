{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXnOjvZgpNSxeACBDZbaor"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN3q17I_1aUp"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import wandb\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "wandb.login()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_data_loaders(image_size,train_data_dir, test_data_dir, data_augmentation):\n",
        "    # Define default transform\n",
        "    default_transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Define transform for data augmentation\n",
        "\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomRotation(degrees=(5,5), fill=(0,)),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size)\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Load the dataset without augmentation\n",
        "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=default_transform)\n",
        "\n",
        "    # Separate classes\n",
        "    class_to_idx = train_dataset.class_to_idx\n",
        "    idx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\n",
        "\n",
        "    # Create a dictionary to hold data indices for each class\n",
        "    class_indices = defaultdict(list)\n",
        "    for idx, (_, label) in enumerate(train_dataset.samples):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    # Set aside 20% of data from each class for validation\n",
        "    validation_indices = []\n",
        "    train_indices = []\n",
        "    for class_idx, indices in class_indices.items():\n",
        "        num_validation = int(0.2 * len(indices))\n",
        "        random.shuffle(indices)  # Shuffle indices to ensure randomness\n",
        "        validation_indices.extend(indices[:num_validation])\n",
        "        train_indices.extend(indices[num_validation:])\n",
        "\n",
        "    # Create PyTorch data loaders for the initial dataset\n",
        "    train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=32, shuffle=True)\n",
        "    validation_loader = DataLoader(Subset(train_dataset, validation_indices), batch_size=32, shuffle=True)\n",
        "\n",
        "    # Load test data\n",
        "    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=default_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # If data augmentation is enabled, create augmented data loader\n",
        "    if data_augmentation:\n",
        "        # Create DataLoader for augmented training data using train_indices\n",
        "        augmented_dataset = datasets.ImageFolder(root=train_data_dir, transform=augment_transform)\n",
        "        augmented_loader = DataLoader(Subset(augmented_dataset, train_indices), batch_size=32, shuffle=True)\n",
        "        # Combine original training data and augmented data\n",
        "\n",
        "        combined_dataset = ConcatDataset([train_loader.dataset, augmented_loader.dataset])\n",
        "        train_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    return train_loader, validation_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_ResNet50(num_classes, batch_norm, dropout_rate, activation, dense_neurons, strategy):\n",
        "    # Define activation function\n",
        "    act_fn = {\n",
        "        'ReLU': nn.ReLU(),\n",
        "        'GELU': nn.GELU(),\n",
        "        'SiLU': nn.SiLU(),\n",
        "        'Mish': nn.Mish()\n",
        "    }[activation]\n",
        "\n",
        "    # Load pre-trained ResNet50 model\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "    # Determine the layers to freeze based on the selected strategy\n",
        "    if strategy == 'feature_extraction':\n",
        "        # Freeze all parameters except the final classifier\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif strategy == 'fine_tuning':\n",
        "        # Fine-tune the entire model No need to freeze any layers\n",
        "        pass\n",
        "    elif strategy == 'fine_tuning_partial':\n",
        "        # Fine-tune up to a certain layer (including the specified layer and all layers above it)\n",
        "        # Here, I fine-tune from layer5 block (5th layer)\n",
        "        unfreeze_from_layer = 5\n",
        "        for idx, (name, param) in enumerate(resnet.named_parameters()):\n",
        "            if idx < unfreeze_from_layer:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "    elif strategy == 'progressive_unfreezing':\n",
        "        # Progressive unfreezing\n",
        "        # Start by freezing all layers except the final classifier\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define the number of layers to unfreeze at each step\n",
        "        num_layers_to_unfreeze = 4\n",
        "\n",
        "        # Get the children of the resnet model\n",
        "        resnet_children = list(resnet.children())\n",
        "\n",
        "        # Loop through each layer and unfreeze progressively\n",
        "        idx = 0\n",
        "        while idx < len(resnet_children):\n",
        "            unfreeze_layers = resnet_children[-(idx + num_layers_to_unfreeze):]\n",
        "            for layer in unfreeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            # Retain gradients of previously unfrozen layers\n",
        "            if idx > 0:\n",
        "                previous_unfreeze_layers = resnet_children[-(idx + num_layers_to_unfreeze + num_layers_to_unfreeze):-(idx + num_layers_to_unfreeze)]\n",
        "                for layer in previous_unfreeze_layers:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = True\n",
        "            idx += num_layers_to_unfreeze\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid strategy. Please choose one of: 'feature_extraction', 'fine_tuning', 'fine_tuning_partial', 'progressive_unfreezing'\")\n",
        "\n",
        "    # Replace the last fully connected layer with a new one that has `dense_neurons` output features\n",
        "    num_ftrs = resnet.fc.in_features\n",
        "    layers = [\n",
        "        nn.Linear(num_ftrs, dense_neurons),\n",
        "        act_fn\n",
        "    ]\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm1d(dense_neurons))\n",
        "    if dropout_rate > 0:\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "    layers.append(nn.Linear(dense_neurons, num_classes))\n",
        "    resnet.fc = nn.Sequential(*layers)\n",
        "\n",
        "    return resnet.to(device)\n"
      ],
      "metadata": {
        "id": "YzqXI5nR1pJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, num_epochs, device):\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        epoch_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "        for images, labels in epoch_progress:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_predictions += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            epoch_progress.set_postfix({'Loss': running_loss / len(train_loader), 'Accuracy': 100 * correct_predictions / total_predictions})\n",
        "        epoch_progress.close()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validation_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        val_loss /= len(validation_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        accuracy = 100 * correct_predictions / total_predictions\n",
        "        loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], accuracy: {accuracy:.2f}, loss: {loss:.2f}, val_accuracy: {val_accuracy:.2f}, val_loss: {val_loss:.2f}\")\n",
        "\n",
        "        # Log to Weights & Biases\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Accuracy\": round(accuracy, 2),\n",
        "            \"Loss\": round(loss, 2),\n",
        "            \"Validation Accuracy\": round(val_accuracy, 2),\n",
        "            \"Validation Loss\": round(val_loss, 2)\n",
        "        })\n",
        "    return model"
      ],
      "metadata": {
        "id": "qnh77OfF1rOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "    print(f\"Accuracy on test set: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "7AQBpbUi1sI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the wandb configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Validation Accuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"activation\": {\"values\": ['ReLU', 'GELU', 'SiLU', 'Mish']},\n",
        "        \"data_augmentation\": {\"values\": ['Yes','No']},\n",
        "        \"batch_normalization\": {\"values\": ['Yes', 'No']},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"dense_neurons\": {\"values\": [128, 256, 512, 1024]},\n",
        "        \"epoch\":{\"values\": [5]},\n",
        "        \"learning_rate\":{\"values\":[0.001,0.0001]},\n",
        "        \"strategy\":{\"values\":['feature_extraction', 'fine_tuning', 'fine_tuning_partial', 'progressive_unfreezing']}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    # Set your hyperparameters from wandb config\n",
        "    config = wandb.config\n",
        "\n",
        "    wandb.run.name = f'ResNet50_activation_{config.activation}_data_augmentation_{config.data_augmentation}_batch_normalization_{config.batch_normalization}_dropout_{config.dropout}_dense_neurons_{config.dense_neurons}_learning_rate_{config.learning_rate}_epoch_{config.epoch}_strategy_{config.strategy}'\n",
        "\n",
        "    data_augmentation=False\n",
        "    if(config.data_augmentation=='Yes'):\n",
        "        data_augmentation=True\n",
        "\n",
        "    batch_normalization=False\n",
        "    if(config.batch_normalization=='Yes'):\n",
        "        batch_normalization=True\n",
        "\n",
        "    # Example usage:\n",
        "    train_data_dir = \"/kaggle/input/nature-12k/inaturalist_12K/train\"\n",
        "    test_data_dir = \"/kaggle/input/nature-12k/inaturalist_12K/val\"\n",
        "    train_loader, validation_loader, test_loader = get_data_loaders(224,train_data_dir, test_data_dir, data_augmentation)\n",
        "\n",
        "    print(f\"Training set size: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation set size: {len(validation_loader.dataset)}\")\n",
        "    print(f\"Test set size: {len(test_loader.dataset)}\")\n",
        "\n",
        "    # Example usage:\n",
        "    model = CNN_ResNet50(10, batch_normalization, config.dropout, config.activation, config.dense_neurons, config.strategy)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    model=train_model(model, train_loader, validation_loader, criterion, optimizer, config.epoch, device)\n",
        "\n",
        "    #Evaluate on Test data\n",
        "    evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "PACLVzv-1ugQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learn_assignment_2\" ,entity=\"cs23m063\")\n",
        "\n",
        "# Run wandb agent to execute the sweep\n",
        "wandb.agent(sweep_id, function=train, count =1)\n"
      ],
      "metadata": {
        "id": "crtKeFrW1yTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}